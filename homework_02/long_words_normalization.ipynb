{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# ... file : longest_words.py\n",
    "#\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ...\n",
    "# ... msds 7337 NLP\n",
    "# ... homework 02\n",
    "# ... gutenberg - documment vocabulary normalization\n",
    "# ... pmcdevitt@smu.edu\n",
    "# ... 15-sep-2018\n",
    "# ...\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... load packages\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.rc('xtick', labelsize=20)     \n",
    "plt.rc('ytick', labelsize=20)\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... some directory and file name definitions\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "files = \".*\\.txt\"\n",
    "home_dir = \"/home/mcdevitt/_ds/_smu/_src/nlp/homework_02/\"\n",
    "corpus_root = \"./text/\"\n",
    "corpus_clean = \"./text_no_license/\"\n",
    "plot_dir = \"./plots/\"\n",
    "\n",
    "os.chdir(home_dir)\n",
    "\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... long_words characterizations\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "def long_words(text_input) :    \n",
    "    text_unique = set(text_input)    \n",
    "    long_words = [w for w in text_unique if len(w) > 7]\n",
    "    longest = sorted(long_words, key = len, reverse = True)[0]\n",
    "    return longest\n",
    "    \n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... remove all non-alpha characters\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "def clean_text(start_text) :\n",
    "    text = [re.sub('^((?![a-zA-Z ]).)*$', '', x) for x in start_text]\n",
    "    text = [re.sub('_', '', x) for x in text]\n",
    "    text = [x.lower() for x in text]\n",
    "    text = [x for x in text if x != '']\n",
    "    return text\n",
    "\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... read in text with gutenberg license removed / assemble corpus for evaluation\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "readers = PlaintextCorpusReader(corpus_clean, files)\n",
    "\n",
    "files = readers.fileids()\n",
    "files[0:10]\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... create table to accumulate summary data\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results_tbl = pd.DataFrame(columns =\n",
    "    ['text_name',\n",
    "     'long_mot',\n",
    "     'long_word_length'])\n",
    "\n",
    "i_index = []\n",
    "i_index = 0\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... loop thru each text to assemble metrics\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "print(\"Some basic statistics\\n\")\n",
    "\n",
    "for fileid in readers.fileids():\n",
    "    rtxt = clean_text(readers.words(fileid))\n",
    "    \n",
    "    le_mot_le_plus_long = long_words(rtxt)\n",
    "    print(\"\\n\", i_index, \"--\", fileid, \" : \", le_mot_le_plus_long, \"\\n\")\n",
    "\n",
    "    table_data = {\n",
    "     'text_name' : fileid,\n",
    "     'long_mot' : le_mot_le_plus_long,\n",
    "     'long_word_length' : len(le_mot_le_plus_long)\n",
    "    } \n",
    "\n",
    "    df_tbl = pd.DataFrame(table_data,\n",
    "        columns = ['text_name',\n",
    "             'long_mot', 'long_word_length'],\n",
    "    index = [i_index + 1])\n",
    "    i_index += 1\n",
    "    results_tbl = results_tbl.append(df_tbl)\n",
    "    \n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... basic statistics table complete\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results_tbl = results_tbl.sort_values(results_tbl.columns[2], ascending = False)\n",
    "results_tbl\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... normalize each column by max column value\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results_nrmlzd = results_tbl.copy()\n",
    "results_nrmlzd = results_nrmlzd.sort_values(results_nrmlzd.columns[2], ascending = False)\n",
    "\n",
    "df = results_nrmlzd.iloc[:, 2]\n",
    "df_nrml = df / df.max()\n",
    "\n",
    "df_labels = results_nrmlzd.iloc[:, 0]\n",
    "df_words =  results_nrmlzd.iloc[:, 1]\n",
    "\n",
    "results_nrmlzd = pd.concat([df_labels, df_words, df_nrml], axis = 1)\n",
    "\n",
    "print('results_nrmlzd - ')\n",
    "\n",
    "results_nrmlzd = results_nrmlzd.sort_values(results_nrmlzd.columns[2], ascending = False)\n",
    "print('results_nrmlzd - sorted by col num_vocab')\n",
    "results_nrmlzd\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... add some shorter title names to fit within plotting space\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results_tbl['title_short'] = [txt[:25] for txt in results_tbl['text_name']]\n",
    "results_tbl['title_short'][:5]\n",
    "\n",
    "results_nrmlzd['title_short'] = [txt[:25] for txt in results_nrmlzd['text_name']]\n",
    "results_nrmlzd['title_short'][:5]\n",
    "\n",
    "results_nrmlzd = results_nrmlzd.sort_values(results_nrmlzd.columns[2], ascending = False)\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot - max word length normalized - sorted in descending order\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "N = len(results_nrmlzd)\n",
    "ind = np.arange(N) \n",
    "width = 0.5\n",
    "\n",
    "_ = plt.figure(figsize = (18, 12))\n",
    "offset = 0\n",
    "_ = plt.bar(ind + offset, results_nrmlzd['long_word_length'],\n",
    "            width,\n",
    "            label='Longest word',\n",
    "            color = 'slateblue')\n",
    "\n",
    "_ = plt.xticks(ind + width / 2, results_nrmlzd['title_short'], fontsize = '15')\n",
    "_ = plt.xticks(rotation=90)\n",
    "_ = plt.legend(loc='upper left')\n",
    "_ = plt.title('Word Lengths - 100+ Readers - Longest Word (normalized)', fontsize = '30')\n",
    "\n",
    "_ = axes = plt.gca()\n",
    "_ = axes.set_ylim([0.4, 1])\n",
    "\n",
    "_ = plt.savefig(plot_dir + 'childrens_books_normalized_longword.png')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... write table to output file for future reference\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "os.chdir(home_dir)\n",
    "\n",
    "file_name = \"max_word_length_normalized.txt\"\n",
    "\n",
    "results_nrmlzd.to_csv(file_name,\n",
    "                      header = True,\n",
    "                      index = None,\n",
    "                      sep=',',\n",
    "                      mode = 'a')\n",
    "    \n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... end_of_file\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
