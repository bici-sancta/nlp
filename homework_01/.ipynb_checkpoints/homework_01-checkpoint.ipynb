{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['literary_world_seventh_reader.txt',\n",
       " 'mcguffey_01.txt',\n",
       " 'mcguffey_02.txt',\n",
       " 'mcguffey_03.txt',\n",
       " 'mcguffey_04.txt',\n",
       " 'mcguffey_05.txt',\n",
       " 'mcguffey_06.txt',\n",
       " 'new_national_first_reader.txt',\n",
       " 'ontario_high_school_reader.txt']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some basic statistics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "plt.rc('xtick', labelsize=20)     \n",
    "plt.rc('ytick', labelsize=20)\n",
    "\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... lexical diversity score - as given in nltk site\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "def lexical_diversity(my_text_data):\n",
    "    tokens = len(my_text_data)\n",
    "    types = len(set(my_text_data))\n",
    "    diversity_score = types / tokens\n",
    "    return diversity_score\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... some directory and file name definitions\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "files = \".*\\.txt\"\n",
    "home_dir = \"/home/mcdevitt/_ds/_smu/msds_7337_nlp/homework_01/\"\n",
    "corpus_root = \"./texts\"\n",
    "plot_dir = \"./plots/\"\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... read in texts / assemble corpus for evaluation\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "readers = PlaintextCorpusReader(corpus_root, files)\n",
    "\n",
    "readers.fileids()\n",
    "\n",
    "corpus = nltk.Text(readers.words())\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... create table to accumulate summary data\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results_tbl = pd.DataFrame(columns =\n",
    "    ['text_name',\n",
    "     'num_chars',\n",
    "     'num_words',\n",
    "     'num_sents',\n",
    "     'num_vocab',\n",
    "     'tokens',\n",
    "     'types',\n",
    "     'lex_div'])\n",
    "\n",
    "i_index = []\n",
    "i_index = 0\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... loop thru each text to assemble metrics\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "print(\"Some basic statistics\\n\")\n",
    "\n",
    "for fileid in readers.fileids():\n",
    "    num_chars = len(readers.raw(fileid))\n",
    "    num_words = len(readers.words(fileid))\n",
    "    num_sents = len(readers.sents(fileid))\n",
    "    tokens = len(readers.words(fileid))\n",
    "    types = len(set(readers.words(fileid)))\n",
    "    num_vocab = len(set(w.lower() for w in readers.words(fileid)))\n",
    "#    print(round(num_chars/num_words, 2),\n",
    "#          round(num_words/num_sents, 2),\n",
    "#          round(num_words/num_vocab, 2), fileid)    \n",
    "    rtxt = readers.words(fileid)\n",
    "    ldiv = lexical_diversity(rtxt)\n",
    "    print(round(ldiv, 4), fileid)\n",
    "\n",
    "    table_data = {\n",
    "     'text_name' : fileid,\n",
    "     'num_chars' : num_chars,\n",
    "     'num_words' : num_words,\n",
    "     'num_sents' : num_sents,\n",
    "     'num_vocab' : num_vocab,\n",
    "     'tokens' : tokens,\n",
    "     'types' : types,\n",
    "     'lex_div' : ldiv\n",
    "    } \n",
    "\n",
    "    df_tbl = pd.DataFrame(table_data,\n",
    "        columns = ['text_name',\n",
    "             'num_chars',\n",
    "             'num_words',\n",
    "             'num_sents',\n",
    "             'num_vocab',\n",
    "             'tokens',\n",
    "             'types',\n",
    "             'lex_div'],\n",
    "    index = [i_index + 1])\n",
    "    i_index += 1\n",
    "    results_tbl = results_tbl.append(df_tbl)\n",
    "\n",
    "results_tbl['vocab_ldiv'] = results_tbl.apply(lambda x: x.lex_div / (x.num_words), axis=1)\n",
    "\n",
    "results_tbl = results_tbl.sort_values(results_tbl.columns[7])\n",
    "print('Results_tbl - sorted by col 8')\n",
    "results_tbl\n",
    "\n",
    "results = results_tbl.copy()\n",
    "\n",
    "results = results.sort_values(results_tbl.columns[7], ascending = False)\n",
    "\n",
    "df = results.iloc[:, 1:8]\n",
    "#df_nrml = (df - df.min()) / (df.max() - df.min())\n",
    "df_nrml = df / df.max()\n",
    "\n",
    "#df_nrml\n",
    "df_labels = results.iloc[:, 0]\n",
    "df_labels\n",
    "\n",
    "results = pd.concat([df_labels, df_nrml], axis = 1)\n",
    "\n",
    "results['vocab_ldiv'] = results.apply(lambda x: x.lex_div * (x.num_words), axis=1)\n",
    "results['vocab_ldiv'] = results['vocab_ldiv'] / results['vocab_ldiv'].max()\n",
    "\n",
    "print('Results - ')\n",
    "results\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot - lexical diversity scores - sorted in ascending TTR\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "N = 9\n",
    "ind = np.arange(N) \n",
    "width = 0.5\n",
    "\n",
    "_ = plt.figure(figsize = (18, 10))\n",
    "offset = 0\n",
    "plt.bar(ind + offset, results_tbl['lex_div'], width, label='Lex_Div', color = 'tomato')\n",
    "\n",
    "plt.xticks(ind + width / 2, results_tbl['text_name'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Lexical Diversity - Selected Readers', fontsize = '30')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 0.2])\n",
    "\n",
    "plt.savefig(plot_dir + 'nltk_readers_ttr.png')\n",
    "plt.show()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot - vocabulary size - sorted in ascending TTR\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "N = 9\n",
    "ind = np.arange(N) \n",
    "width = 0.4\n",
    "\n",
    "_ = plt.figure(figsize = (18, 10))\n",
    "offset = width / 2\n",
    "plt.bar(ind + offset, results_tbl['num_vocab'], width, label='Vocab', color = 'orchid')\n",
    "plt.bar(ind + offset + width, results_tbl['types'], width, label='Types', color = 'cornflowerblue')\n",
    "\n",
    "plt.xticks(ind + width / 2, results_tbl['text_name'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Vocabulary Size - Selected Readers', fontsize = '30')\n",
    "\n",
    "axes = plt.gca()\n",
    "#axes.set_ylim([0, 0.2])\n",
    "\n",
    "plt.savefig(plot_dir + 'nltk_readers_vocab.png')\n",
    "plt.show()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot - vocabulary size - sorted in ascending TTR\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "N = 9\n",
    "ind = np.arange(N) \n",
    "width = 0.4\n",
    "\n",
    "_ = plt.figure(figsize = (18, 10))\n",
    "offset = width / 2\n",
    "plt.scatter(results_tbl['num_vocab'], np.log10(results_tbl['types']), color = 'orchid', s = 100)\n",
    "plt.scatter(results_tbl['num_vocab'], np.log10(results_tbl['tokens']), color = 'slateblue', s = 100)\n",
    "plt.scatter(results_tbl['num_vocab'], np.log10(results_tbl['num_sents']), color = 'cornflowerblue', s = 100)\n",
    "plt.scatter(results_tbl['num_vocab'], np.log10(results_tbl['num_chars']), color = 'darkcyan', s = 100)\n",
    "\n",
    "#plt.xticks(ind + width / 2, results_tbl['text_name'])\n",
    "#plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left', fontsize = '25')\n",
    "plt.title('Vocabulary Size - Selected Readers', fontsize = '30')\n",
    "plt.xlabel('Number of Vocabulary Words', fontsize = '25')\n",
    "plt.ylabel('Corresponding Statistics (log10 scale)', fontsize = '25')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([1, 7])\n",
    "\n",
    "plt.savefig(plot_dir + 'nltk_metrics_vs_vocab.png')\n",
    "plt.show()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot comparison of all (normalized) metrics - sorted in descending TTR\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "results = results.sort_values(results_tbl.columns[7], ascending = False)\n",
    "N = 9\n",
    "ind = np.arange(N) \n",
    "width = 0.143\n",
    "\n",
    "_ = plt.figure(figsize = (18, 8))\n",
    "offset = width / 2\n",
    "plt.bar(ind + offset, results['lex_div'], width, label='Lex_Div', color = 'tomato')\n",
    "plt.bar(ind + offset + width, results['num_chars'], width, label='Chars', color = 'dodgerblue')\n",
    "plt.bar(ind + offset + width*2, results['num_words'], width, label='Words', color = 'slateblue')\n",
    "plt.bar(ind + offset + width*3, results['num_sents'], width, label='Sentences', color = 'cornflowerblue')\n",
    "plt.bar(ind + offset + width*4, results['num_vocab'], width, label='Vocab', color = 'orchid')\n",
    "plt.bar(ind + offset + width*5, results['vocab_ldiv'], width, label='Vocab_LDiv', color = 'darkcyan')\n",
    "\n",
    "plt.xticks(ind + width / 2, results['text_name'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Normalized Characteristics Comparison (Lex Div Sorted)', fontsize = '30')\n",
    "\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1, 1.1])\n",
    "\n",
    "plt.savefig(plot_dir + 'nltk_readers_ttr_normalized.png')\n",
    "plt.show()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... plot comparison of all (normalized) metrics - \n",
    "# ... sorted in descending TTR*num_tokens\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "\n",
    "results = results.sort_values(results_tbl.columns[8], ascending = False)\n",
    "N = 9\n",
    "ind = np.arange(N) \n",
    "width = 0.143\n",
    "\n",
    "_ = plt.figure(figsize = (18, 8))\n",
    "\n",
    "offset = width / 2\n",
    "plt.bar(ind + offset, results['lex_div'], width, label='Lex_Div', color = 'tomato')\n",
    "plt.bar(ind + offset + width, results['num_chars'], width, label='Chars', color = 'dodgerblue', alpha = 0.9)\n",
    "plt.bar(ind + offset + width*2, results['num_words'], width, label='Words', color = 'slateblue', alpha = 0.9)\n",
    "plt.bar(ind + offset + width*3, results['num_sents'], width, label='Sentences', color = 'cornflowerblue', alpha = 0.9)\n",
    "plt.bar(ind + offset + width*4, results['num_vocab'], width, label='Vocab', color = 'orchid', alpha = 0.9)\n",
    "plt.bar(ind + offset + width*5, results['vocab_ldiv'], width, label='Vocab_LDiv', color = 'c')\n",
    "\n",
    "plt.xticks(ind + width / 2, results['text_name'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Normalized Characteristics Comparison (Lex Div * Vocab)', fontsize = '30')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1, 1.1])\n",
    "\n",
    "plt.savefig(plot_dir + 'nltk_readers_ttrxtokens_normalized.png')\n",
    "plt.show()\n",
    "\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
    "# ... end_of_file\n",
    "# ... -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
